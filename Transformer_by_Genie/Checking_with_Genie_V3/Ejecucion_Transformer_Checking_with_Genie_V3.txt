(TENSORFLOW_2025) PS C:\Users\gtoma\Downloads>  & 'c:\Users\gtoma\anaconda3\envs\TENSORFLOW_2025\python.exe' 'c:\Users\gtoma\.vscode\extensions\ms-python.debugpy-2025.8.0-win32-x64\bundled\libs\debugpy\launcher' '63624' '--' 'c:\Users\gtoma\Downloads\Checking_with_Genie_V3.py' 
2025-06-09 19:41:12.258692: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-06-09 19:41:13.187654: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
Libraries imported successfully.
Training samples: 25000
Test samples: 25000
Vocabulary size: 20000
Vocabulary size with CLS token: 20001
Max sequence length: 200
WARNING:tensorflow:From c:\Users\gtoma\anaconda3\envs\TENSORFLOW_2025\lib\site-packages\keras\src\backend\tensorflow\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

2025-06-09 19:41:18.200067: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Model: "functional_4"
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓
┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩
│ input_layer (InputLayer)             │ (None, 200)                 │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ token_and_position_embedding         │ (None, 200, 128)            │       2,585,728 │
│ (TokenAndPositionEmbedding)          │                             │                 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ transformer_block (TransformerBlock) │ (None, 200, 128)            │         297,344 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ transformer_block_1                  │ (None, 200, 128)            │         297,344 │
│ (TransformerBlock)                   │                             │                 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ transformer_block_2                  │ (None, 200, 128)            │         297,344 │
│ (TransformerBlock)                   │                             │                 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ transformer_block_3                  │ (None, 200, 128)            │         297,344 │
│ (TransformerBlock)                   │                             │                 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ lambda (Lambda)                      │ (None, 128)                 │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dropout_12 (Dropout)                 │ (None, 128)                 │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dense_8 (Dense)                      │ (None, 20)                  │           2,580 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dropout_13 (Dropout)                 │ (None, 20)                  │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dense_9 (Dense)                      │ (None, 1)                   │              21 │
└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘
 Total params: 3,777,705 (14.41 MB)
 Trainable params: 3,777,705 (14.41 MB)
 Non-trainable params: 0 (0.00 B)
Epoch 1/10
2500/2500 ━━━━━━━━━━━━━━━━━━━━ 162s 63ms/step - accuracy: 0.5051 - loss: 0.7046 - val_accuracy: 0.5062 - val_loss: 0.6931
Epoch 2/10
2500/2500 ━━━━━━━━━━━━━━━━━━━━ 504s 202ms/step - accuracy: 0.4915 - loss: 0.6935 - val_accuracy: 0.5062 - val_loss: 0.6931
Epoch 3/10
2500/2500 ━━━━━━━━━━━━━━━━━━━━ 597s 215ms/step - accuracy: 0.4987 - loss: 0.6932 - val_accuracy: 0.4938 - val_loss: 0.6933
Epoch 4/10
2500/2500 ━━━━━━━━━━━━━━━━━━━━ 314s 116ms/step - accuracy: 0.4966 - loss: 0.6932 - val_accuracy: 0.5062 - val_loss: 0.6931
Epoch 5/10
2500/2500 ━━━━━━━━━━━━━━━━━━━━ 449s 167ms/step - accuracy: 0.5024 - loss: 0.6932 - val_accuracy: 0.4938 - val_loss: 0.6932
Epoch 6/10
2500/2500 ━━━━━━━━━━━━━━━━━━━━ 239s 96ms/step - accuracy: 0.4961 - loss: 0.6932 - val_accuracy: 0.4938 - val_loss: 0.6934
Epoch 7/10
2500/2500 ━━━━━━━━━━━━━━━━━━━━ 310s 124ms/step - accuracy: 0.5102 - loss: 0.6931 - val_accuracy: 0.5062 - val_loss: 0.6931
Epoch 8/10
2500/2500 ━━━━━━━━━━━━━━━━━━━━ 312s 120ms/step - accuracy: 0.5028 - loss: 0.6932 - val_accuracy: 0.4938 - val_loss: 0.6932
Epoch 9/10
2500/2500 ━━━━━━━━━━━━━━━━━━━━ 151s 60ms/step - accuracy: 0.4982 - loss: 0.6932 - val_accuracy: 0.4938 - val_loss: 0.6934
Epoch 10/10
2500/2500 ━━━━━━━━━━━━━━━━━━━━ 396s 158ms/step - accuracy: 0.5007 - loss: 0.6933 - val_accuracy: 0.4938 - val_loss: 0.6932
782/782 ━━━━━━━━━━━━━━━━━━━━ 98s 124ms/step - accuracy: 0.4927 - loss: 0.6932 
Test Loss: 0.6931515336036682
Test Accuracy: 0.5
782/782 ━━━━━━━━━━━━━━━━━━━━ 40s 51ms/step  
Matriz de confusión:
[[    0 12500]
 [    0 12500]]
(TENSORFLOW_2025) PS C:\Users\gtoma\Downloads> 
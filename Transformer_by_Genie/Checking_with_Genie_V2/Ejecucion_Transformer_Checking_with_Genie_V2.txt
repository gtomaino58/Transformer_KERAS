PS C:\Users\gtoma\Downloads>  & 'c:\Users\gtoma\anaconda3\envs\TENSORFLOW_2025\python.exe' 'c:\Users\gtoma\.vscode\extensions\ms-python.debugpy-2025.8.0-win32-x64\bundled\libs\debugpy\launcher' '61491' '--' 'c:\Users\gtoma\Downloads\Checking_with_Genie_V2.py'
2025-06-09 18:06:22.167325: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-06-09 18:06:23.049003: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
Libraries imported successfully.
Training samples: 25000
Test samples: 25000
Vocabulary size: 20000
Vocabulary size with CLS token: 20001
Max sequence length: 200
WARNING:tensorflow:From c:\Users\gtoma\anaconda3\envs\TENSORFLOW_2025\lib\site-packages\keras\src\backend\tensorflow\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

2025-06-09 18:06:29.578516: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Model: "functional_4"
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓
┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩
│ input_layer (InputLayer)             │ (None, 200)                 │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ token_and_position_embedding         │ (None, 200, 128)            │       2,585,728 │
│ (TokenAndPositionEmbedding)          │                             │                 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ transformer_block (TransformerBlock) │ (None, 200, 128)            │         297,344 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ transformer_block_1                  │ (None, 200, 128)            │         297,344 │
│ (TransformerBlock)                   │                             │                 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ transformer_block_2                  │ (None, 200, 128)            │         297,344 │
│ transformer_block_2                  │ (None, 200, 128)            │         297,344 │
│ (TransformerBlock)                   │                             │                 │
│ (TransformerBlock)                   │                             │                 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ transformer_block_3                  │ (None, 200, 128)            │         297,344 │
│ (TransformerBlock)                   │                             │                 │
│ (TransformerBlock)                   │                             │                 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ lambda (Lambda)                      │ (None, 128)                 │               0 │
│ lambda (Lambda)                      │ (None, 128)                 │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dropout_12 (Dropout)                 │ (None, 128)                 │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dense_8 (Dense)                      │ (None, 20)                  │           2,580 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dropout_13 (Dropout)                 │ (None, 20)                  │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dense_9 (Dense)                      │ (None, 1)                   │              21 │
└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘
 Total params: 3,777,705 (14.41 MB)
 Trainable params: 3,777,705 (14.41 MB)
 Non-trainable params: 0 (0.00 B)
Epoch 1/10
2500/2500 ━━━━━━━━━━━━━━━━━━━━ 364s 144ms/step - accuracy: 0.4907 - loss: 0.6984 - val_accuracy: 0.5062 - val_loss: 0.6931
Epoch 2/10
2500/2500 ━━━━━━━━━━━━━━━━━━━━ 344s 138ms/step - accuracy: 0.4954 - loss: 0.6933 - val_accuracy: 0.5062 - val_loss: 0.6931
Epoch 3/10
2500/2500 ━━━━━━━━━━━━━━━━━━━━ 158s 63ms/step - accuracy: 0.5033 - loss: 0.6932 - val_accuracy: 0.5062 - val_loss: 0.6931
Epoch 4/10
2500/2500 ━━━━━━━━━━━━━━━━━━━━ 257s 103ms/step - accuracy: 0.4904 - loss: 0.6933 - val_accuracy: 0.4938 - val_loss: 0.6934
Epoch 5/10
2500/2500 ━━━━━━━━━━━━━━━━━━━━ 419s 166ms/step - accuracy: 0.4952 - loss: 0.6932 - val_accuracy: 0.4938 - val_loss: 0.6934
Epoch 6/10
2500/2500 ━━━━━━━━━━━━━━━━━━━━ 313s 114ms/step - accuracy: 0.5040 - loss: 0.6933 - val_accuracy: 0.5062 - val_loss: 0.6931
Epoch 7/10
2500/2500 ━━━━━━━━━━━━━━━━━━━━ 245s 98ms/step - accuracy: 0.4887 - loss: 0.6932 - val_accuracy: 0.5062 - val_loss: 0.6931
Epoch 8/10
2500/2500 ━━━━━━━━━━━━━━━━━━━━ 497s 192ms/step - accuracy: 0.4921 - loss: 0.6932 - val_accuracy: 0.4938 - val_loss: 0.6934
Epoch 9/10
2500/2500 ━━━━━━━━━━━━━━━━━━━━ 512s 196ms/step - accuracy: 0.5036 - loss: 0.6933 - val_accuracy: 0.4938 - val_loss: 0.6932
Epoch 10/10
2500/2500 ━━━━━━━━━━━━━━━━━━━━ 290s 111ms/step - accuracy: 0.5056 - loss: 0.6932 - val_accuracy: 0.4938 - val_loss: 0.6932
782/782 ━━━━━━━━━━━━━━━━━━━━ 52s 67ms/step - accuracy: 0.4927 - loss: 0.6932  
Test Loss: 0.6931491494178772
Test Accuracy: 0.5
PS C:\Users\gtoma\Downloads> 